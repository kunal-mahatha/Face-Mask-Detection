{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "import cv2 as cv\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2 \n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from imutils.video import VideoStream\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '/Users/kunal/Desktop/ML projects/dataset'\n",
    "imagePaths = list(paths.list_images(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagePaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in imagePaths:\n",
    "    label = i.split(os.path.sep)[-2]\n",
    "    labels.append(label)\n",
    "    image = load_img(i, target_size = (224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    data.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data, dtype = 'float32')\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(data, labels, test_size = 0.20, random_state = 10, stratify = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(rotation_range=20, zoom_range= 0.15, width_shift_range= 0.2, shear_range= 0.15, horizontal_flip= True, vertical_flip= True, fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel = MobileNetV2(weights='imagenet', include_top=False, input_tensor= Input(shape = (224,224,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size = (7,7))(headModel)\n",
    "headModel = Flatten(name = 'Flatten')(headModel)\n",
    "headModel = Dense(128, activation = 'relu')(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation = 'softmax')(headModel)\n",
    "\n",
    "model = Model(inputs = baseModel.input, outputs = headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "Epochs = 20\n",
    "batch_size = 12\n",
    "\n",
    "opt = Adam(lr=learning_rate, decay = learning_rate/Epochs)\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "\n",
    "H = model.fit(\n",
    "    aug.flow(train_X, train_Y, batch_size = batch_size),\n",
    "    steps_per_epoch=len(train_X)//batch_size,\n",
    "    validation_data=(test_X, test_Y),\n",
    "    validation_steps=len(test_X)//batch_size,\n",
    "    epochs = Epochs\n",
    ")\n",
    "\n",
    "model.save('mask.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_X, batch_size=batch_size)\n",
    "predict = np.argmax(predict, axis = 1)\n",
    "print(classification_report(test_Y.argmax(axis=1), predict, target_names = lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('mask.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototxtPath = os.path.sep.join(['/Users/kunal/Desktop/ML projects/deploy.prototxt'])\n",
    "weightsPath = os.path.sep.join(['/Users/kunal/Desktop/ML projects/res10_300x300_ssd_iter_140000.caffemodel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceNet = cv.dnn.readNet(prototxtPath, weightsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_mask(frame, faceNet, model):\n",
    "    #Take dimensions to make blob\n",
    "    (h,w) = frame.shape[:2]\n",
    "    blob = cv.dnn.blobFromImage(frame, 1.0, (300,300), (104.0, 177.0, 123.0))\n",
    "    \n",
    "    faceNet.setInput(blob)\n",
    "    detections = faceNet.forward()\n",
    "    \n",
    "    #initialise the list of faces, their correspondng locations and list of predictiob\n",
    "    faces = []\n",
    "    locs = []\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(0, detections.shape[2]):\n",
    "        confidence = detections[0,0,i,2]\n",
    "        \n",
    "        if confidence > 0.5:\n",
    "            #coordinates of x and y\n",
    "            box = detections[0,0,i,3:7]*np.array([w,h,w,h])\n",
    "            (startX, startY, endX, endY) = box.astype('int')\n",
    "            \n",
    "            #ensure the bounding boxes fall within the dimension of the frame\n",
    "            (startX, startY) = (max(0, startX), max(0, startY))\n",
    "            (endX, endY) = (min(w-1, endX), min(h-1, endY))\n",
    "            \n",
    "            #convert ROI from BGR to RGB channel, resize ut to 224, 224 and preprocess it\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            face = cv.cvtColor(face, cv.COLOR_BGR2RGB)\n",
    "            face = cv.resize(face, (224, 224))\n",
    "            face = img_to_array(face)\n",
    "            face = preprocess_input(face)\n",
    "            \n",
    "            faces.append(face)\n",
    "            locs.append((startX, startY, endX, endY))\n",
    "            \n",
    "            \n",
    "        if len(faces)>0:\n",
    "            faces = np.array(faces, dtype = 'float32')\n",
    "            preds = model.predict(faces, batch_size = 12)\n",
    "            \n",
    "        return (locs, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-39p1qqfs/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3a055f6d8ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#detect faces and mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaceNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#loop over detected face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-97631f7dd146>\u001b[0m in \u001b[0;36mdetect_mask\u001b[0;34m(frame, faceNet, model)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m#convert ROI from BGR to RGB channel, resize ut to 224, 224 and preprocess it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstartY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mendY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mendX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.1) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-39p1qqfs/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "vs = VideoStream(src = 0).start()\n",
    "\n",
    "while True:\n",
    "    #resize the frames\n",
    "    frame = vs.read()\n",
    "    frane = imutils.resize(frame, width = 400)\n",
    "    \n",
    "    #detect faces and mask\n",
    "    (locs, preds) = detect_mask(frame, faceNet, model)\n",
    "    \n",
    "    #loop over detected face \n",
    "    for (box, pred) in zip(locs, preds):\n",
    "        (startX, startY, endX, endY) = box\n",
    "        (mask, withoutMask) = pred\n",
    "        \n",
    "         #color and lobel of the bonding box and text\n",
    "        label = 'Mask' if mask>withoutMask else 'No Mask'\n",
    "        color = (0,255, 0) if label == 'Mask' else (0, 0, 255)\n",
    "            \n",
    "        #display the label and bonding boxes\n",
    "        cv.putText(frame, label, (startX, startY-10), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            \n",
    "        cv.rectangle(frame,(startX, startY), (endX, endY), 2)\n",
    "            \n",
    "    cv.imshow('Frame', frame)\n",
    "    key = cv.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key ==ord('q'):\n",
    "        break\n",
    "    \n",
    "cv.destroyAllWindows\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
